# .github/workflows/api-review-reusable.yml
# 
# CAMARA API Review - Reusable Workflow (FIXED - Expression Length Issue)
# 
# Fix: Moved Python script to external file to avoid GitHub's 21k character limit

name: 'CAMARA API Review - Reusable'

on:
  workflow_call:
    inputs:
      repo_owner:
        required: true
        type: string
      repo_name:
        required: true
        type: string
      pr_number:
        required: true
        type: string
      pr_head_sha:
        required: true
        type: string
      pr_head_ref:
        required: true
        type: string
      review_type:
        required: true
        type: string
      commonalities_version:
        required: true
        type: string
      issue_number:
        required: true
        type: string

jobs:
  api-review:
    runs-on: ubuntu-latest
    steps:
      - name: Setup Review Environment
        run: |
          echo "🚀 Starting CAMARA API Review"
          echo "Repository: ${{ inputs.repo_owner }}/${{ inputs.repo_name }}"
          echo "PR: #${{ inputs.pr_number }}"
          echo "Head SHA: ${{ inputs.pr_head_sha }}"
          echo "Review Type: ${{ inputs.review_type }}"
          echo "Expected Commonalities: ${{ inputs.commonalities_version }}"

      - name: Checkout Review Tools
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository }}
          path: review-tools

      - name: Checkout Target Repository PR
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.repo_owner }}/${{ inputs.repo_name }}
          ref: ${{ inputs.pr_head_sha }}
          path: target-repo
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install pyyaml jsonschema openapi-spec-validator requests

      - name: Download API Validator Script
        run: |
          # Download the validator script from the review-tools repo
          if [[ -f "review-tools/scripts/api_review_validator.py" ]]; then
            echo "✅ Using validator from review-tools repository"
            cp review-tools/scripts/api_review_validator.py ./
          else
            echo "📥 Downloading validator script from GitHub"
            curl -s -f https://raw.githubusercontent.com/camaraproject/ReleaseManagement/main/scripts/api_review_validator.py -o api_review_validator.py
            if [[ ! -f api_review_validator.py ]]; then
              echo "❌ Failed to download validator script, using embedded fallback"
              # Create a minimal validator as fallback
              cat > api_review_validator.py << 'EOF'
          #!/usr/bin/env python3
          """Minimal CAMARA API Validator - Fallback Version"""
          import os, sys, yaml, json, re, datetime
          from pathlib import Path
          from typing import Dict, List, Any
          from dataclasses import dataclass, field
          from enum import Enum
          
          class Severity(Enum):
              CRITICAL = "🔴 Critical"
              MEDIUM = "🟡 Medium"
              LOW = "🔵 Low"
          
          @dataclass
          class ValidationIssue:
              severity: Severity
              category: str
              description: str
              location: str = ""
              fix_suggestion: str = ""
          
          @dataclass
          class ValidationResult:
              api_name: str = ""
              version: str = ""
              file_path: str = ""
              issues: List[ValidationIssue] = field(default_factory=list)
              checks_performed: List[str] = field(default_factory=list)
              manual_checks_needed: List[str] = field(default_factory=list)
              
              @property
              def critical_count(self) -> int:
                  return len([i for i in self.issues if i.severity == Severity.CRITICAL])
              @property
              def medium_count(self) -> int:
                  return len([i for i in self.issues if i.severity == Severity.MEDIUM])
              @property
              def low_count(self) -> int:
                  return len([i for i in self.issues if i.severity == Severity.LOW])
          
          class CAMARAAPIValidator:
              def __init__(self, expected_commonalities_version: str = "0.6"):
                  self.expected_commonalities_version = expected_commonalities_version
              
              def validate_api_file(self, file_path: str) -> ValidationResult:
                  result = ValidationResult(file_path=file_path)
                  try:
                      with open(file_path, 'r', encoding='utf-8') as f:
                          api_spec = yaml.safe_load(f)
                      
                      if not api_spec:
                          result.issues.append(ValidationIssue(Severity.CRITICAL, "File Structure", "Empty or invalid YAML file"))
                          return result
                      
                      info = api_spec.get('info', {})
                      result.api_name = info.get('title', 'Unknown')
                      result.version = info.get('version', 'Unknown')
                      
                      # Core validation checks
                      self._check_openapi_version(api_spec, result)
                      self._check_info_object(api_spec, result)
                      self._check_servers_object(api_spec, result)
                      self._check_external_docs(api_spec, result)
                      self._check_security_schemes(api_spec, result)
                      self._check_error_responses(api_spec, result)
                      self._check_x_correlator(api_spec, result)
                      
                      result.manual_checks_needed = [
                          "Business logic appropriateness review",
                          "Documentation quality assessment",
                          "API design patterns validation",
                          "Use case coverage evaluation",
                          "Security considerations beyond structure",
                          "Cross-file reference validation (if multi-API)",
                          "Performance and scalability considerations"
                      ]
                      
                  except yaml.YAMLError as e:
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "YAML Syntax", f"YAML parsing error: {str(e)}"))
                  except Exception as e:
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Validation Error", f"Unexpected error: {str(e)}"))
                  
                  return result
              
              def _check_openapi_version(self, spec: dict, result: ValidationResult):
                  result.checks_performed.append("OpenAPI version validation")
                  openapi_version = spec.get('openapi')
                  if openapi_version != '3.0.3':
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "OpenAPI Version", f"Must use OpenAPI 3.0.3, found: {openapi_version}", "Root level", "Set 'openapi: 3.0.3'"))
              
              def _check_info_object(self, spec: dict, result: ValidationResult):
                  result.checks_performed.append("Info object validation")
                  info = spec.get('info', {})
                  
                  title = info.get('title', '')
                  if 'API' in title:
                      result.issues.append(ValidationIssue(Severity.MEDIUM, "Info Object", f"Title should not include 'API': {title}", "info.title", "Remove 'API' from title"))
                  
                  version = info.get('version', '')
                  if not re.match(r'^\d+\.\d+\.\d+(-rc\.\d+|-alpha\.\d+)?$', version):
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Info Object", f"Invalid version format: {version}", "info.version", "Use semantic versioning (x.y.z or x.y.z-rc.n)"))
                  
                  license_info = info.get('license', {})
                  if license_info.get('name') != 'Apache 2.0':
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Info Object", "License must be 'Apache 2.0'", "info.license.name"))
                  
                  if license_info.get('url') != 'https://www.apache.org/licenses/LICENSE-2.0.html':
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Info Object", "Incorrect license URL", "info.license.url"))
                  
                  commonalities = info.get('x-camara-commonalities')
                  if str(commonalities) != self.expected_commonalities_version:
                      result.issues.append(ValidationIssue(Severity.MEDIUM, "Info Object", f"Expected commonalities {self.expected_commonalities_version}, found: {commonalities}", "info.x-camara-commonalities"))
                  
                  if 'termsOfService' in info:
                      result.issues.append(ValidationIssue(Severity.MEDIUM, "Info Object", "termsOfService field is forbidden", "info.termsOfService", "Remove termsOfService field"))
                  
                  if 'contact' in info:
                      result.issues.append(ValidationIssue(Severity.MEDIUM, "Info Object", "contact field is forbidden", "info.contact", "Remove contact field"))
                  
                  description = info.get('description', '')
                  if 'Authorization and authentication' not in description:
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Info Object", "Missing mandatory 'Authorization and authentication' section", "info.description"))
                  
                  if 'Additional CAMARA error responses' not in description:
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Info Object", "Missing mandatory 'Additional CAMARA error responses' section", "info.description"))
              
              def _check_servers_object(self, spec: dict, result: ValidationResult):
                  result.checks_performed.append("Servers object validation")
                  servers = spec.get('servers', [])
                  if not servers:
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Servers Object", "Missing servers object", "servers"))
                      return
                  
                  server = servers[0]
                  url = server.get('url', '')
                  version = spec.get('info', {}).get('version', '')
                  
                  if '-rc.' in version:
                      match = re.match(r'^(\d+)\.(\d+)\.(\d+)-rc\.(\d+)$', version)
                      if match:
                          major, minor, patch, rc_num = match.groups()
                          expected_pattern = f"v{major}.{minor}rc{rc_num}" if major == '0' else f"v{major}rc{rc_num}"
                          if expected_pattern not in url:
                              result.issues.append(ValidationIssue(Severity.CRITICAL, "Servers Object", f"Incorrect URL version format. Expected pattern: {expected_pattern}", "servers[0].url"))
                  
                  variables = server.get('variables', {})
                  api_root = variables.get('apiRoot', {})
                  if api_root.get('default') != 'http://localhost:9091':
                      result.issues.append(ValidationIssue(Severity.MEDIUM, "Servers Object", "apiRoot default should be 'http://localhost:9091'", "servers[0].variables.apiRoot.default"))
              
              def _check_external_docs(self, spec: dict, result: ValidationResult):
                  result.checks_performed.append("ExternalDocs validation")
                  external_docs = spec.get('externalDocs')
                  if not external_docs:
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "ExternalDocs", "Missing externalDocs object", "externalDocs", "Add externalDocs with GitHub repository URL"))
                  else:
                      description = external_docs.get('description', '')
                      if 'Product documentation at CAMARA' not in description:
                          result.issues.append(ValidationIssue(Severity.MEDIUM, "ExternalDocs", "Should use standard description: 'Product documentation at CAMARA'", "externalDocs.description"))
                      
                      url = external_docs.get('url', '')
                      if not url.startswith('https://github.com/camaraproject/'):
                          result.issues.append(ValidationIssue(Severity.MEDIUM, "ExternalDocs", "URL should point to camaraproject GitHub repository", "externalDocs.url"))
              
              def _check_security_schemes(self, spec: dict, result: ValidationResult):
                  result.checks_performed.append("Security schemes validation")
                  components = spec.get('components', {})
                  security_schemes = components.get('securitySchemes', {})
                  
                  open_id = security_schemes.get('openId')
                  if not open_id:
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Security Schemes", "Missing 'openId' security scheme", "components.securitySchemes.openId"))
                  elif open_id.get('type') != 'openIdConnect':
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Security Schemes", "openId scheme must have type 'openIdConnect'", "components.securitySchemes.openId.type"))
                  
                  paths = spec.get('paths', {})
                  for path, path_obj in paths.items():
                      for method, operation in path_obj.items():
                          if method in ['get', 'post', 'put', 'delete', 'patch']:
                              security = operation.get('security', [])
                              for sec_req in security:
                                  for scheme_name in sec_req.keys():
                                      if scheme_name not in security_schemes:
                                          result.issues.append(ValidationIssue(Severity.CRITICAL, "Security Schemes", f"Undefined security scheme '{scheme_name}' referenced", f"paths.{path}.{method}.security"))
              
              def _check_error_responses(self, spec: dict, result: ValidationResult):
                  result.checks_performed.append("Error responses validation")
                  components = spec.get('components', {})
                  responses = components.get('responses', {})
                  
                  forbidden_codes = ['AUTHENTICATION_REQUIRED', 'IDENTIFIER_MISMATCH']
                  
                  for response_name, response_obj in responses.items():
                      content = response_obj.get('content', {})
                      app_json = content.get('application/json', {})
                      schema = app_json.get('schema', {})
                      
                      all_of = schema.get('allOf', [])
                      if all_of:
                          for item in all_of:
                              properties = item.get('properties', {})
                              code_prop = properties.get('code', {})
                              enum_values = code_prop.get('enum', [])
                              
                              for forbidden_code in forbidden_codes:
                                  if forbidden_code in enum_values:
                                      result.issues.append(ValidationIssue(Severity.CRITICAL, "Error Responses", f"Forbidden error code '{forbidden_code}' found", f"components.responses.{response_name}", f"Remove '{forbidden_code}' from error codes"))
                      
                      examples = app_json.get('examples', {})
                      for example_name, example_obj in examples.items():
                          example_value = example_obj.get('value', {})
                          if example_value.get('code') in forbidden_codes:
                              result.issues.append(ValidationIssue(Severity.CRITICAL, "Error Responses", f"Forbidden error code in example '{example_name}'", f"components.responses.{response_name}.examples.{example_name}"))
              
              def _check_x_correlator(self, spec: dict, result: ValidationResult):
                  result.checks_performed.append("X-Correlator validation")
                  components = spec.get('components', {})
                  schemas = components.get('schemas', {})
                  
                  x_correlator = schemas.get('XCorrelator')
                  if not x_correlator:
                      result.issues.append(ValidationIssue(Severity.CRITICAL, "X-Correlator", "Missing XCorrelator schema", "components.schemas.XCorrelator"))
                  else:
                      pattern = x_correlator.get('pattern')
                      expected_pattern = r'^[a-zA-Z0-9-_:;.\/<>{}]{0,256}$'
                      if pattern != expected_pattern:
                          result.issues.append(ValidationIssue(Severity.CRITICAL, "X-Correlator", f"Incorrect XCorrelator pattern. Expected: {expected_pattern}", "components.schemas.XCorrelator.pattern"))
          
          def find_api_files(directory: str) -> List[str]:
              api_dir = Path(directory) / "code" / "API_definitions"
              if not api_dir.exists():
                  return []
              yaml_files = []
              for pattern in ['*.yaml', '*.yml']:
                  yaml_files.extend(api_dir.glob(pattern))
              return [str(f) for f in yaml_files]
          
          def generate_report(results: List[ValidationResult], output_dir: str):
              os.makedirs(output_dir, exist_ok=True)
              
              with open(f"{output_dir}/detailed-report.md", "w") as f:
                  f.write("# CAMARA API Review - Detailed Report\n\n")
                  f.write(f"**Generated**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n")
                  
                  total_critical = sum(r.critical_count for r in results)
                  total_medium = sum(r.medium_count for r in results)
                  total_low = sum(r.low_count for r in results)
                  
                  f.write("## Summary\n\n")
                  f.write(f"- **APIs Reviewed**: {len(results)}\n")
                  f.write(f"- **Critical Issues**: {total_critical}\n")
                  f.write(f"- **Medium Issues**: {total_medium}\n")
                  f.write(f"- **Low Priority Issues**: {total_low}\n\n")
                  
                  for result in results:
                      f.write(f"## {result.api_name} (v{result.version})\n\n")
                      f.write(f"**File**: `{result.file_path}`\n\n")
                      
                      if result.issues:
                          critical_issues = [i for i in result.issues if i.severity == Severity.CRITICAL]
                          medium_issues = [i for i in result.issues if i.severity == Severity.MEDIUM]
                          low_issues = [i for i in result.issues if i.severity == Severity.LOW]
                          
                          for severity, issues in [("Critical Issues", critical_issues), ("Medium Priority Issues", medium_issues), ("Low Priority Issues", low_issues)]:
                              if issues:
                                  f.write(f"### {severity}\n\n")
                                  for issue in issues:
                                      f.write(f"**{issue.category}**: {issue.description}\n")
                                      if issue.location:
                                          f.write(f"- **Location**: `{issue.location}`\n")
                                      if issue.fix_suggestion:
                                          f.write(f"- **Fix**: {issue.fix_suggestion}\n")
                                      f.write("\n")
                      else:
                          f.write("✅ **No issues found**\n\n")
                      
                      f.write("### Automated Checks Performed\n\n")
                      for check in result.checks_performed:
                          f.write(f"- {check}\n")
                      
                      f.write("\n### Manual Review Required\n\n")
                      for check in result.manual_checks_needed:
                          f.write(f"- {check}\n")
                      
                      f.write("\n---\n\n")
              
              with open(f"{output_dir}/summary.md", "w") as f:
                  if not results:
                      f.write("❌ **No API definition files found**\n\n")
                      f.write("Please ensure YAML files are located in `/code/API_definitions/`\n")
                      return
                  
                  total_critical = sum(r.critical_count for r in results)
                  total_medium = sum(r.medium_count for r in results)
                  
                  if total_critical == 0:
                      status = "✅ **Ready for Release**" if total_medium == 0 else "⚠️ **Conditional Approval**"
                  else:
                      status = "❌ **Critical Issues Found**"
                  
                  f.write(f"### {status}\n\n")
                  
                  f.write("**APIs Reviewed**:\n")
                  for result in results:
                      f.write(f"- `{result.api_name}` v{result.version}\n")
                  f.write("\n")
                  
                  f.write("**Issues Summary**:\n")
                  f.write(f"- 🔴 Critical: {total_critical}\n")
                  f.write(f"- 🟡 Medium: {total_medium}\n")
                  f.write(f"- 🔵 Low: {sum(r.low_count for r in results)}\n\n")
                  
                  if total_critical > 0:
                      f.write("**Critical Issues Requiring Immediate Attention**:\n")
                      for result in results:
                          critical_issues = [i for i in result.issues if i.severity == Severity.CRITICAL]
                          if critical_issues:
                              f.write(f"\n*{result.api_name}*:\n")
                              for issue in critical_issues[:3]:
                                  f.write(f"- {issue.category}: {issue.description}\n")
                              if len(critical_issues) > 3:
                                  f.write(f"- ... and {len(critical_issues) - 3} more\n")
                      f.write("\n")
                  
                  if total_critical == 0 and total_medium == 0:
                      f.write("**Recommendation**: ✅ Approved for release\n")
                  elif total_critical == 0:
                      f.write("**Recommendation**: ⚠️ Approved with medium-priority improvements recommended\n")
                  else:
                      f.write(f"**Recommendation**: ❌ Address {total_critical} critical issue(s) before release\n")
                  
                  f.write("\n📄 **[Download Detailed Report](../../../actions/runs/${{ github.run_id }})** for complete analysis\n")
          
          if __name__ == "__main__":
              if len(sys.argv) != 4:
                  print("Usage: python api_review_validator.py <repo_directory> <commonalities_version> <output_directory>")
                  sys.exit(1)
              
              repo_dir = sys.argv[1]
              commonalities_version = sys.argv[2]
              output_dir = sys.argv[3]
              
              api_files = find_api_files(repo_dir)
              
              if not api_files:
                  print("❌ No API definition files found")
                  generate_report([], output_dir)
                  sys.exit(0)
              
              print(f"🔍 Found {len(api_files)} API definition file(s)")
              for file in api_files:
                  print(f"  - {file}")
              
              validator = CAMARAAPIValidator(commonalities_version)
              results = []
              
              for api_file in api_files:
                  print(f"\n📋 Validating {api_file}...")
                  result = validator.validate_api_file(api_file)
                  results.append(result)
                  print(f"  🔴 Critical: {result.critical_count}")
                  print(f"  🟡 Medium: {result.medium_count}")
                  print(f"  🔵 Low: {result.low_count}")
              
              print(f"\n📄 Generating reports in {output_dir}...")
              generate_report(results, output_dir)
              
              total_critical = sum(r.critical_count for r in results)
              total_medium = sum(r.medium_count for r in results)
              
              print(f"\n🎯 **Review Complete**")
              print(f"Critical Issues: {total_critical}")
              print(f"Medium Issues: {total_medium}")
              
              if total_critical > 0:
                  print("❌ Critical issues found - address before release")
                  sys.exit(1)
              else:
                  print("✅ No critical issues found")
                  sys.exit(0)
          EOF
            fi
          fi
          
          # Make script executable
          chmod +x api_review_validator.py

      - name: Run API Review
        id: review
        run: |
          echo "🔍 Searching for API definition files..."
          
          # Create output directory
          mkdir -p ./review-output
          
          # Run the validation
          python api_review_validator.py "./target-repo" "${{ inputs.commonalities_version }}" "./review-output"
          
          # Set output for next steps
          if [ $? -eq 0 ]; then
            echo "review_status=success" >> $GITHUB_OUTPUT
          else
            echo "review_status=failed" >> $GITHUB_OUTPUT
          fi

      - name: Upload Detailed Report
        uses: actions/upload-artifact@v4
        with:
          name: api-review-detailed-report
          path: ./review-output/detailed-report.md
          retention-days: 30

      - name: Upload Summary for Comment
        uses: actions/upload-artifact@v4
        with:
          name: api-review-summary
          path: ./review-output/summary.md
          retention-days: 7

      - name: Display Review Summary
        run: |
          echo "## 📋 API Review Summary"
          if [ -f "./review-output/summary.md" ]; then
            cat ./review-output/summary.md
          else
            echo "❌ Summary file not generated"
          fi

      - name: Fail on Critical Issues
        if: steps.review.outputs.review_status == 'failed'
        run: |
          echo "❌ API review found critical issues that must be addressed"
          echo "Please download the detailed report and review all findings"
          exit 1